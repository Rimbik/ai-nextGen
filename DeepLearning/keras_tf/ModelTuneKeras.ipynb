{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGkqvNnyLQUG"
      },
      "outputs": [],
      "source": [
        "###Early stopping\n",
        "\n",
        "# Early stopping is a technique in Keras to halt training once a monitored metric (like validation loss or accuracy) stops improving.\n",
        "# It prevents overfitting and saves time by avoiding unnecessary training. Here's how you can implement it:\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',    # Metric to monitor (e.g., 'val_loss' or 'val_accuracy')\n",
        "    patience=3,            # Number of epochs to wait before stopping if no improvement\n",
        "    restore_best_weights=True  # Restores the weights from the epoch with the best monitored value\n",
        ")\n",
        "\n",
        "# Fit the model with the callback\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_val, y_val),  # Validation data for monitoring\n",
        "    epochs=50,                      # Maximum number of epochs\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop]          # Add EarlyStopping to the training process\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d1IX7AHLby2"
      },
      "outputs": [],
      "source": [
        "### Adding Batch Normalization\n",
        "\n",
        "# Batch normalization is a technique used to normalize inputs to a layer during training, which can improve training speed and stability. Here's a quick guide to implementing batch normalization in Keras:\n",
        "# typically applied after the layer's computation but before the activation function.\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Activation\n",
        "\n",
        "# Define a simple Sequential model\n",
        "model = Sequential([\n",
        "    Dense(64, input_shape=(784,)),  # Fully connected layer\n",
        "    BatchNormalization(),          # Add Batch Normalization\n",
        "    Activation('relu'),            # Activation after Batch Normalization\n",
        "    Dense(10, activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summarize the model\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5wCw9mTLt1K"
      },
      "outputs": [],
      "source": [
        "### Regularization:\n",
        "\n",
        "\t# 1. L1 and L2 Regularization\n",
        "\t# \tThese add a penalty based on the size of the model weights:\n",
        "\n",
        "\t# \tL1 Regularization: Encourages sparsity by adding penalties proportional to the absolute value of the weights.\n",
        "\t# \tL2 Regularization: Penalizes large weights by adding penalties proportional to the square of the weights (also called Ridge regularization).\n",
        "\n",
        "    from tensorflow.keras.models import Sequential\n",
        "\tfrom tensorflow.keras.layers import Dense\n",
        "\tfrom tensorflow.keras.regularizers import l1, l2\n",
        "\n",
        "\tmodel = Sequential([\n",
        "\t\tDense(64, activation='relu', kernel_regularizer=l2(0.01), input_shape=(100,)),\n",
        "\t\tDense(32, activation='relu', kernel_regularizer=l1(0.01)),\n",
        "\t\tDense(1, activation='sigmoid')\n",
        "\t])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2QCcTGNL4_A"
      },
      "outputs": [],
      "source": [
        "### Dropout\n",
        "# Dropout randomly disables a fraction of neurons during training to prevent co-dependence between units.\n",
        "# It's commonly used in fully connected and convolutional layers.\n",
        "\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),  # Drop 50% of neurons\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),  # Drop 30% of neurons\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiXJ926jMIcP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
