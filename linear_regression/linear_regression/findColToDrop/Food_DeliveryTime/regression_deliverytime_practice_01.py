# -*- coding: utf-8 -*-
"""Regression_DeliveryTime_Practice_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MzVZF3OehiFLxJmfaIQjyEmISvRXFmob
"""

import pandas as pd
from io import StringIO
import io
import requests
import json
from io import FileIO
from json import loads, dumps
# import csv
import matplotlib.pyplot as plt

url = "https://raw.githubusercontent.com/Rimbik/ai-nextGen/refs/heads/main/linear_regression/sample_dataset/Food_Delivery_Times.csv"
s = requests.get(url).content
df = pd.read_csv(io.StringIO(s.decode('utf-8')))

df.head()

### Needed to deal with Str fields in X (independent variables)
#one hot encoding using OneHotEncoder of Scikit-Learn
def one_hot_encoder(df):
    # Create a OneHotEncoder object
    import pandas as pd
    from sklearn.preprocessing import OneHotEncoder

    df = pd.DataFrame(df)

    #Extract categorical columns from the dataframe
    #Here we extract the columns with object datatype as they are the categorical columns
    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()
    encoder = OneHotEncoder(sparse_output=False)

    # Apply one-hot encoding to the categorical columns
    one_hot_encoded = encoder.fit_transform(df[categorical_columns])

    #Create a DataFrame with the one-hot encoded columns
    #We use get_feature_names_out() to get the column names for the encoded data
    one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))

    # Concatenate the one-hot encoded dataframe with the original dataframe
    df_encoded = pd.concat([df, one_hot_df], axis=1)

    # Drop the original categorical columns
    df_encoded = df_encoded.drop(categorical_columns, axis=1)
    # print(f"Encoded Employee data : \n{df_encoded}")

    return df_encoded

df_encoded = one_hot_encoder(df)


threshold = len(df_encoded) * 0.1
# Drop columns where the number of null values is greater than the threshold
# df_encoded = df_encoded.dropna(axis=1, thresh=threshold)
df_encoded = df_encoded.dropna(axis=1)

df_encoded.isnull().values.any()
df_encoded

# VIF Technique (Variance Inflation Factor - VIF)
from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd

def calculate_vif(X):
    vif_data = pd.DataFrame()

    vif_data["feature"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return vif_data

vif_df = calculate_vif(df_encoded.drop(columns=['Order_ID','Delivery_Time_min']))

# drop features with high VIF values
# RULES : to identify high VIF
  # A) VIF > 10: Serious multicollinearity
  # B) VIF > 4 or 5: Moderate to high multicollinearity
  # C) VIF = 1: The predictor is not correlated with other variables

# Find >>
# A : features above 10 are considered to indicate serious multicollinearity
    # Price = This we will predict
    # serious multicollinearity [Year_Built, Square_Feet, Number_Bedroom]
# B : features that are greater than 4 or 5
    # [Garage_Size, Num_Bedrooms, Num_Floors, Num_Bathrooms]
# VIF = 1: The predictor is not correlated with other variables
    # Nil

# ---------------------------------------------------------------------------#
# if you get VIF = inf >> it indicates a perfect multicollinearity issue
vif_df = vif_df.sort_values(by='VIF', ascending=False)
vif_df



# Progrmatically identify colums to be dropped
def tag_columns_tobeDropped(df):
    mydf = df.copy()

    vif_df = calculate_vif(mydf)
    vif_df["candrop"] = ""
    vif_df["drop_reason"] = "-"

    vif_df = vif_df.sort_values(by='VIF', ascending=False)

    for index, row in vif_df.iterrows():
          if row["VIF"] > 10:
              vif_df.at[index, "candrop"] = "Y"
              vif_df.at[index, "drop_reason"] = "Serious multicollinearity"

          elif row["VIF"] > 5:
              vif_df.at[index, "candrop"] = "Y"
              vif_df.at[index, "drop_reason"] = "Moderate to high multicollinearity"

          elif row["VIF"] == 1:
              vif_df.at[index, "candrop"] = "Y"
              vif_df.at[index, "drop_reason"] = "Not correlated with other variables"

          # elif row["VIF"] == df.inf:
          #     vif_df.at[index, "candrop"] = "Y"
          #     vif_df.at[index, "drop_reason"] = "perfect multicollinearity issue"

          else:
              vif_df.at[index, "candrop"] = "N"
              vif_df.at[index, "drop_reason"] = "No need to drop"

    return vif_df

vif_df_indicator = tag_columns_tobeDropped(df_encoded)
vif_df_indicator

high_vif_features   = vif_df_indicator[vif_df_indicator["candrop"] == "Y"]
features_canbetaken = vif_df_indicator[vif_df_indicator["candrop"] == "N"]

high_vif_features = high_vif_features["feature"].tolist()
features_canbetaken = features_canbetaken["feature"].tolist()

features_canbetaken = features_canbetaken
print(features_canbetaken)

model = df.filter(features_canbetaken, axis=1)
model = model.drop('Order_ID', axis=1)
model.head() # final model to Train

df_encoded.plot.scatter(x='Distance_km', y='Delivery_Time_min')
df_encoded.plot.scatter(x='Preparation_Time_min', y='Delivery_Time_min')

X = df_encoded.drop(columns=['Delivery_Time_min'])
y = df['Delivery_Time_min']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=11) #20% test Data
# use random_state to have the same selection in the record set everytime you run.

len(X_train), len(y_train),len(X_test),len(y_test)

from sklearn.linear_model import LinearRegression
clf = LinearRegression()
clf.fit(X_train,y_train)

LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1)

clf.score(X_test,y_test)